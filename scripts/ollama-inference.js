#!/usr/bin/env node

/**
 * Ollama Inference Module
 * Clean interface for generating content via Ollama
 * Handles connection, error handling, and fallback
 */

const fetch = require('node-fetch');

const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'mistral';

class OllamaInference {
  constructor(url = OLLAMA_URL, model = OLLAMA_MODEL) {
    this.url = url;
    this.model = model;
    this.timeout = 600000; // 10 minutes for generation (slower on CI)
  }

  /**
   * Check if Ollama service is available
   */
  async isAvailable() {
    try {
      const response = await fetch(`${this.url}/api/tags`, { 
        timeout: 5000 
      });
      return response.ok;
    } catch (error) {
      return false;
    }
  }

  /**
   * Generate content using Ollama
   */
  async generate(prompt, options = {}) {
    const {
      temperature = 0.7,
      topP = 0.9,
      topK = 40,
      numPredict = 2048,
      verbose = true
    } = options;

    try {
      if (verbose) {
        console.log(`üì° Connecting to Ollama at ${this.url}...`);
        console.log(`ü§ñ Model: ${this.model}`);
      }

      const response = await fetch(`${this.url}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: this.model,
          prompt: prompt,
          stream: false,
          options: {
            temperature,
            top_p: topP,
            top_k: topK,
            num_predict: numPredict
          }
        }),
        timeout: this.timeout
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();

      if (!data.response) {
        throw new Error('Empty response from Ollama');
      }

      if (verbose) {
        console.log('‚úÖ Content generated successfully');
      }

      return {
        success: true,
        content: data.response,
        model: data.model,
        tokens: data.eval_count || 0
      };
    } catch (error) {
      if (verbose) {
        console.error('‚ùå Ollama generation failed:', error.message);
      }

      return {
        success: false,
        error: error.message,
        content: null
      };
    }
  }

  /**
   * Generate with streaming output (for real-time feedback)
   */
  async generateStream(prompt, onChunk, options = {}) {
    const {
      temperature = 0.7,
      topP = 0.9,
      topK = 40,
      numPredict = 2048
    } = options;

    try {
      console.log(`üì° Connecting to Ollama at ${this.url}...`);
      console.log(`ü§ñ Model: ${this.model}`);

      const response = await fetch(`${this.url}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: this.model,
          prompt: prompt,
          stream: true,
          options: {
            temperature,
            top_p: topP,
            top_k: topK,
            num_predict: numPredict
          }
        }),
        timeout: this.timeout
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      let fullContent = '';
      const text = await response.text();
      const lines = text.trim().split('\n');

      for (const line of lines) {
        if (line.trim()) {
          try {
            const json = JSON.parse(line);
            if (json.response) {
              fullContent += json.response;
              if (onChunk) {
                onChunk(json.response);
              }
            }
          } catch (e) {
            // Ignore parse errors
          }
        }
      }

      return {
        success: true,
        content: fullContent
      };
    } catch (error) {
      console.error('‚ùå Stream generation failed:', error.message);
      return {
        success: false,
        error: error.message,
        content: null
      };
    }
  }

  /**
   * Check model availability
   */
  async modelExists() {
    try {
      const response = await fetch(`${this.url}/api/tags`);
      if (!response.ok) {
        return false;
      }

      const data = await response.json();
      const models = data.models || [];
      return models.some(m => m.name.includes(this.model));
    } catch (error) {
      return false;
    }
  }

  /**
   * Get available models
   */
  async listModels() {
    try {
      const response = await fetch(`${this.url}/api/tags`);
      if (!response.ok) {
        throw new Error('Failed to list models');
      }

      const data = await response.json();
      return data.models || [];
    } catch (error) {
      console.error('Error listing models:', error.message);
      return [];
    }
  }

  /**
   * Pull a model
   */
  async pullModel(modelName = this.model) {
    try {
      console.log(`üì¶ Pulling model: ${modelName}...`);

      const response = await fetch(`${this.url}/api/pull`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name: modelName }),
        timeout: 600000 // 10 minutes for pulling
      });

      if (!response.ok) {
        throw new Error(`Failed to pull model: HTTP ${response.status}`);
      }

      const text = await response.text();
      const lines = text.trim().split('\n');

      for (const line of lines) {
        try {
          const json = JSON.parse(line);
          if (json.status) {
            process.stdout.write(`\rüì¶ ${json.status}`);
          }
        } catch (e) {
          // Ignore parse errors
        }
      }

      console.log('\n‚úÖ Model pulled successfully');
      return true;
    } catch (error) {
      console.error('\n‚ùå Failed to pull model:', error.message);
      throw error;
    }
  }
}

// Export for use in other modules
module.exports = OllamaInference;
