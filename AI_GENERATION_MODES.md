# AI Generation Modes

autonomousBLOG supports three modes of article generation, with different quality vs. complexity tradeoffs.

## 1. Fallback Content (No AI Required) ‚ö°

**What it is:** Hardcoded templates in `scripts/generate-article-ollama.js`

**How it works:**
```javascript
const sections = {
  'news': [
    '## Background\n\nTo understand why this matters...',
    '## Key Developments\n\nSeveral key factors...',
    // etc
  ]
};
```

**Pros:**
- ‚úÖ No dependencies, no setup required
- ‚úÖ Always works, no failures
- ‚úÖ Instant generation
- ‚úÖ No API keys or credentials

**Cons:**
- ‚ùå Same content for every article (just topic name changes)
- ‚ùå Not truly AI-generated
- ‚ùå Boring, repetitive structure
- ‚ùå No unique insights

**When used:**
- When Ollama is not available
- Default fallback if AI fails

**Example output:**
```
# Your Topic Here

*This article was autonomously generated by autonomousBLOG.*

In today's rapidly evolving landscape of [category], Your Topic Here has emerged...

## Background
## Key Developments  
## Implications
## Conclusion
```

---

## 2. Ollama (Local AI) ü§ñ

**What it is:** Local LLM running on your machine via Ollama

**Setup:**
```bash
# Install Ollama from https://ollama.ai
ollama serve
ollama pull mistral  # or any model
```

**How it works:**
- Sends prompt to local Ollama API
- Gets back unique AI-generated content
- No internet required (after initial setup)

**Pros:**
- ‚úÖ Real AI-generated content (unique per article)
- ‚úÖ Runs locally, no API keys needed
- ‚úÖ Fast once model is loaded (1-3 min per article)
- ‚úÖ Good quality with mistral/neural-chat
- ‚úÖ Privacy-friendly

**Cons:**
- ‚ö†Ô∏è Setup takes 5-10 minutes (first run)
- ‚ö†Ô∏è Requires 4-26GB disk space (model dependent)
- ‚ö†Ô∏è Slower than commercial APIs
- ‚ö†Ô∏è GitHub Actions setup is complex

**When used:**
- When `OLLAMA_URL` points to running instance
- Default in workflow if local Ollama is available

**Recommended models:**
- `mistral` (4.4GB, fast, good quality) ‚≠ê
- `neural-chat` (3.8GB, very fast)
- `dolphin-mixtral` (26GB, best quality)

**Command:**
```bash
node scripts/generate-article-ollama.js
```

**Example output:**
Unique, contextual articles with proper structure:
```
# The Rise of AI in Modern Development

AI has revolutionized how we approach software development...
[Your unique content here]
```

---

## 3. GPT4All (Lightweight Local AI) üöÄ

**What it is:** Minimal local inference engine (C++ based)

**Setup:**
```bash
# Download GPT4All from https://gpt4all.io
# Or run via Docker
docker run -p 4891:4891 -v ~/.cache/gpt4all:/root/.cache/gpt4all nomic-ai/gpt4all

# Then pull a model
# (GPT4All includes lightweight models by default)
```

**How it works:**
- Much faster than Ollama
- Smaller memory footprint
- Similar to Ollama but optimized for speed

**Pros:**
- ‚úÖ Faster than Ollama (30s-2min per article)
- ‚úÖ Smaller models (1-7GB)
- ‚úÖ Real AI-generated content
- ‚úÖ Lower resource requirements
- ‚úÖ Better for GitHub Actions

**Cons:**
- ‚ö†Ô∏è Slightly lower quality than large models
- ‚ö†Ô∏è Less documentation than Ollama

**When used:**
- When speed matters
- When disk space is limited
- Recommended for production

**Command:**
```bash
GPTALL_URL=http://localhost:4891 node scripts/generate-with-gpt4all.js
```

---

## Comparison Table

| Feature | Fallback | Ollama | GPT4All |
|---------|----------|--------|---------|
| **Setup Time** | None | 5-10 min | 2-5 min |
| **Disk Space** | None | 4-26GB | 1-7GB |
| **Speed** | Instant | 1-3 min | 30s-2min |
| **Quality** | Poor | Excellent | Good |
| **Uniqueness** | None | High | High |
| **API Keys** | None | None | None |
| **Cost** | Free | Free | Free |
| **Privacy** | ‚úÖ | ‚úÖ | ‚úÖ |
| **Internet Required** | No | No | No |

---

## How to Switch Modes

### Use Ollama (Default)
No setup needed - workflow tries Ollama automatically.

To enable:
```bash
ollama serve &
ollama pull mistral
```

### Use GPT4All
1. Install GPT4All or run via Docker
2. Update workflow to use GPT4All script

### Use Fallback
Already automatic when AI is unavailable. To force it:
- Remove/don't run Ollama
- Workflow will fallback to templates

---

## Recommended Setup

### Development
Use **Ollama** locally for testing:
```bash
ollama serve
node scripts/generate-article-ollama.js
```

### Production (GitHub Actions)
Use **GPT4All** or **Remote Ollama**:
```yaml
- name: Generate article
  env:
    GPTALL_URL: http://your-gpt4all-server:4891
  run: node scripts/generate-with-gpt4all.js
```

### Lightweight/Cloud
Use **Remote Ollama** with ngrok:
```yaml
- name: Generate article
  env:
    OLLAMA_URL: https://your-ngrok-url.ngrok.io
  run: node scripts/generate-article-ollama.js
```

---

## Future Options

Could add support for:
- OpenAI API (requires keys, paid)
- Claude API (requires keys, paid)
- LLaMA 2 (larger, higher quality)
- Phi-3 (tiny, optimized for edge)

All current options are **free and open-source**.
